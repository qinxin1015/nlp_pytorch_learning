{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第1步：导入豆瓣电影数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TorchText中的一个重要概念是`Field`。`Field`决定了你的数据会被怎样处理。在我们的情感分类任务中，我们所需要接触到的数据有文本字符串和两种情感，\"pos\"或者\"neg\"。\n",
    "- `Field`的参数制定了数据会被怎样处理。\n",
    "- 我们使用`TEXT` field来定义如何处理电影评论，使用`LABEL` field来处理两个情感类别。\n",
    "- 我们的`TEXT` field带有`tokenize='spacy'`，这表示我们会用[spaCy](https://spacy.io) tokenizer来tokenize英文句子。如果我们不特别声明`tokenize`这个参数，那么默认的分词方法是使用空格。\n",
    "- 安装spaCy\n",
    "```\n",
    "pip install -U torchtext\n",
    "pip install -U spacy==3.0.6\n",
    "pip install en_core_web_sm.3.0.0.tar.gz\n",
    "```\n",
    "- `LABEL`由`LabelField`定义。这是一种特别的用来处理label的`Field`。我们后面会解释dtype。\n",
    "- 更多关于`Fields`，参见https://github.com/pytorch/text/blob/master/torchtext/data/field.py\n",
    "- 和之前一样，我们会设定random seeds使实验可以复现。\n",
    "\n",
    "- TorchText支持很多常见的自然语言处理数据集。\n",
    "- 下面的代码会自动下载IMDb数据集，然后分成train/test两个`torchtext.datasets`类别。数据被前面的`Fields`处理。IMDb数据集一共有50000电影评论，每个评论都被标注为正面的或负面的。\n",
    "\n",
    "<b>先了解下Spacy库：[spaCy介绍和使用教程](https://juejin.im/post/5971a4b9f265da6c42353332?utm_source=gold_browser_extension%5D)</b></font>  \n",
    "<b>再了解下torchtext库：[torchtext介绍和使用教程](https://blog.csdn.net/u012436149/article/details/79310176)：这个新手必看，不看下面代码听不懂</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c049ec97bf0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data,datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED) #为CPU设置随机种子\n",
    "torch.cuda.manual_seed(SEED)#为GPU设置随机种子\n",
    "torch.backends.cudnn.deterministic = True  #在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销。\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, n_filters, filter_sizes, \n",
    "                 output_size, dropout, pad_idx):\n",
    "        super(CNN,self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels = 1, out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_size)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        self.linear = nn.Linear(len(filter_sizes) * n_filters, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # [seq_len, batch_size] => [batch_size, seq_len]\n",
    "        text = text.permute(1, 0) \n",
    "        # [batch_size, seq_len] => [batch_size, seq_len, embedding_size]\n",
    "        embeded = self.embed(text) \n",
    "        # [batch_size, seq_len, embedding_size] => [batch_size, 1, seq_len, embedding_size]\n",
    "        embeded = embeded.unsqueeze(1) \n",
    "        # [batch_size, 1, seq_len, embedding_size] => [batch_size, n_filters, seq_len-filter_size+1, 1] \n",
    "        #                                          => [batch_size, n_filters, seq_len-filter_size+1]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]    \n",
    "        # [batch_size, n_filters, seq_len-filter_size+1] => [batch_size, n_filters, 1] \n",
    "        #                                                => [batch_size, n_filters]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  \n",
    "        # [batch_size, n_filters] => [batch_size, len(filter_sizes)*n_filters]\n",
    "        concat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # [batch_size, len(filter_sizes)*n_filters] => [batch_size, 1]\n",
    "        out = self.linear(concat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     11,
     29
    ]
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y): #计算准确率\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    #.round函数：四舍五入\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss,epoch_acc,total_len = 0.,0.,0.\n",
    "    #有时候训练时会用到dropout、归一化等方法，但是测试的时候不能用dropout等方法。 \n",
    "    model.train() #这步一定要加，是为了区分model训练和测试的模式的。\n",
    "    for batch in iterator:\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() #反向传播\n",
    "        optimizer.step() #梯度下降\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "    return epoch_loss / total_len, epoch_acc / total_len\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss,epoch_acc,total_len = 0.,0.,0.\n",
    "    model.eval()\n",
    "    #转换成测试模式，冻结dropout层或其他层。\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator: \n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            epoch_loss += loss.item() * len(batch.label)\n",
    "            epoch_acc += acc.item() * len(batch.label)\n",
    "            total_len += len(batch.label)\n",
    "    model.train() #调回训练模式   \n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy') \n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "# 划分 训练集，验证集，测试集\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED)) #默认split_ratio=0.7\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size=25000, \n",
    "                 vectors=\"glove.6B.100d\", \n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data) \n",
    "\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "                                        (train_data, valid_data, test_data), \n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = 100\n",
    "NUM_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_SIZE = 1\n",
    "DROPOUT = 0.5\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "N_EPOCHS = 10\n",
    "\n",
    "model = CNN(INPUT_SIZE, \n",
    "            EMBEDDING_SIZE, \n",
    "            NUM_FILTERS, \n",
    "            FILTER_SIZES, \n",
    "            OUTPUT_SIZE, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr = LEARNING_RATE) #定义优化器\n",
    "criterion = nn.BCEWithLogitsLoss()  #定义损失函数，这个BCEWithLogitsLoss特殊情况，二分类损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'CNN-model.pt')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "model.load_state_dict(torch.load('CNN-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "import spacy  #分词工具，跟NLTK类似\n",
    "nlp = spacy.load('en')\n",
    "def predict_sentiment(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]#分词\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized] \n",
    "    #sentence的索引\n",
    "    tensor = torch.LongTensor(indexed).to(device) #seq_len\n",
    "    tensor = tensor.unsqueeze(1) \n",
    "    #seq_len * batch_size（1）\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    #tensor与text一样的tensor\n",
    "    return prediction.item()\n",
    "\n",
    "predict_sentiment(\"I love This film bad \")\n",
    "predict_sentiment(\"This film is great\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
