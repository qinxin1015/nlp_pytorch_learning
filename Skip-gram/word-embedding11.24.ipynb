{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二课 词向量\n",
    "\n",
    "第二课学习目标\n",
    "- 学习词向量的概念\n",
    "- 用Skip-thought模型训练词向量\n",
    "- 学习使用PyTorch dataset和dataloader\n",
    "- 学习定义PyTorch模型\n",
    "- 学习torch.nn中常见的Module\n",
    "    - Embedding\n",
    "- 学习常见的PyTorch operations\n",
    "    - bmm\n",
    "    - logsigmoid\n",
    "- 保存和读取PyTorch模型\n",
    "    \n",
    "\n",
    "第二课使用的训练数据可以从以下链接下载到。\n",
    "\n",
    "链接:https://pan.baidu.com/s/1tFeK3mXuVXEy3EMarfeWvg  密码:v2z5\n",
    "\n",
    "在这一份notebook中，我们会（尽可能）尝试复现论文[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)中训练词向量的方法. 我们会实现Skip-gram模型，并且使用论文中noice contrastive sampling的目标函数。\n",
    "\n",
    "这篇论文有很多模型实现的细节，这些细节对于词向量的好坏至关重要。我们虽然无法完全复现论文中的实验结果，主要是由于计算资源等各种细节原因，但是我们还是可以大致展示如何训练词向量。\n",
    "\n",
    "以下是一些我们没有实现的细节\n",
    "- subsampling：参考论文section 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch\\package\\_directory_reader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy.core._multiarray_umath' (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\00 PERSONAL-LEARNING\\python_learn\\第二课资料 \n",
      " D:\\00 PERSONAL-LEARNING\\python_learn \n",
      " D:\\00 PERSONAL-LEARNING\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "from torch.nn.parameter import Parameter\n",
    "   \n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity # 计算余弦相似度\n",
    "import os, sys\n",
    "pwd = os.getcwd()\n",
    "father_path = os.path.abspath(os.path.dirname(pwd)+os.path.sep+\".\")\n",
    "grader_father = os.path.abspath(os.path.dirname(pwd)+os.path.sep+\"..\")\n",
    "sys.path.append(father_path)\n",
    "sys.path.append(grader_father)\n",
    "print(pwd,\"\\n\",father_path,\"\\n\",grader_father)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值\n",
    "random.seed(53113)\n",
    "np.random.seed(53113)\n",
    "torch.manual_seed(53113)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(53113)\n",
    "    \n",
    "# 设定一些超参数\n",
    "BATCH_SIZE = 128 # the batch size\n",
    "LEARNING_RATE = 0.2 # the initial learning rate\n",
    "\n",
    "K = 100 # number of negative samples\n",
    "C = 3 # nearby words threshold，一般设置3~5\n",
    "NUM_EPOCHS = 2 # The number of epochs of training\n",
    "MAX_VOCAB_SIZE = 30000 # the vocabulary size\n",
    "EMBEDDING_SIZE = 100\n",
    "     \n",
    "LOG_FILE = \"word-embedding.log\"\n",
    "\n",
    "# tokenize函数，把一篇文本转化成一个个单词\n",
    "def word_tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 根据语料,创建 vocabulary\n",
    "### 2. 为vocabulary的每一个词创建index({word:index})\n",
    "### 3. 计算每个词出现的freqs, 并进行归一化,用于负采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从文本文件中读取所有的文字，通过这些文本创建一个vocabulary\n",
    "- 由于单词数量可能太大，我们只选取最常见的MAX_VOCAB_SIZE个单词\n",
    "- 我们添加一个UNK单词表示所有不常见的单词\n",
    "- 我们需要记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalized) frequency，以及单词总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\00 PERSONAL-LEARNING\\python_learn\\第二课资料\\data/text8/text8.train.txt\n"
     ]
    }
   ],
   "source": [
    "# 数据集路劲\n",
    "TRAIN_PATH = os.path.join(pwd,\"data/text8/text8.train.txt\")\n",
    "SIMLEX_PATH = os.path.join(pwd,\"data/simlex-999.txt\")\n",
    "MEN_PATH = os.path.join(pwd,\"data/men.txt\")\n",
    "SIM353 = os.path.join(pwd,\"data/wordsim353.csv\")\n",
    "print(TRAIN_PATH)\n",
    "\n",
    "with open(TRAIN_PATH, \"r\") as fin:\n",
    "    text = fin.read()\n",
    "    \n",
    "text = [w for w in word_tokenize(text.lower())]\n",
    "# 统计每个词在语料中出现的次数，选择最出现频率最高的 MAX_VOCAB_SIZE-1 个词\n",
    "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE-1))\n",
    "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))# 留一个位置给“<unk>”\n",
    "'''论文中指出，所有词的输入都要转化为数字（index）  word_to_idx'''\n",
    "# 列表类型，word\n",
    "idx_to_word = [word for word in vocab.keys()] \n",
    "# 字典类型，{word：index} 记录下 word在语料中的位置（index），机器只能处理数字，不能处理单词\n",
    "word_to_idx = {word:i for i, word in enumerate(idx_to_word)}\n",
    "''' 论文中，需要计算词的freqs,用于负采样 '''\n",
    "word_counts = np.array([count for count in vocab.values()], dtype=np.float32)\n",
    "word_freqs = word_counts / np.sum(word_counts) # 归一化\n",
    "# 可以增加freqs小的词的freqs, 降低freqs大的词的freqs. 这样就可以增加freqs小的词(重要的词)的采样可能性\n",
    "word_freqs = word_freqs ** (3./4.)   \n",
    "word_freqs = word_freqs / np.sum(word_freqs) # 用来做 negative sampling\n",
    "'''重置VOCAB_SIZE,以防止语料库大小大于VOCAB_SIZE指定大小的情况'''\n",
    "VOCAB_SIZE = len(idx_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现Dataloader\n",
    "\n",
    "一个dataloader需要以下内容：\n",
    "\n",
    "- 把所有text编码成数字，然后用subsampling预处理这些文字。\n",
    "- 保存vocabulary，单词count，normalized word frequency\n",
    "- 每个iteration sample一个中心词\n",
    "- 根据当前的中心词返回context单词\n",
    "- 根据中心词sample一些negative单词\n",
    "- 返回单词的counts\n",
    "\n",
    "这里有一个好的tutorial介绍如何使用[PyTorch dataloader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "为了使用dataloader，我们需要定义以下两个function:\n",
    "\n",
    "- ```__len__``` function需要返回整个数据集中有多少个item\n",
    "- ```__get__``` 根据给定的index返回一个item\n",
    "\n",
    "有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     2,
     22
    ]
   },
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(tud.Dataset):\n",
    "    def __init__(self, text, VOCAB_SIZE, word_to_idx, idx_to_word, word_freqs):\n",
    "        ''' text: a list of words, all text from the training dataset\n",
    "            word_to_idx: the dictionary from word to idx\n",
    "            idx_to_word: idx to word mapping\n",
    "            word_freq: the frequency of each word\n",
    "            word_counts: the word counts\n",
    "        '''\n",
    "        super(WordEmbeddingDataset, self).__init__()\n",
    "        self.text_encoded = [word_to_idx.get(t, VOCAB_SIZE-1) for t in text]\n",
    "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的(positive)单词\n",
    "            - 随机采样的K个单词作为negative sample\n",
    "        '''\n",
    "        center_word = self.text_encoded[idx]\n",
    "        pos_indices = list(range(idx-C, idx)) + list(range(idx+1, idx+C+1))\n",
    "        pos_indices = [i%len(self.text_encoded) for i in pos_indices]\n",
    "        pos_words = self.text_encoded[pos_indices] \n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0])\n",
    "        return center_word, pos_words, neg_words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建dataset和dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义PyTorch模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        ''' 初始化输出和输出embedding\n",
    "        '''\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        \n",
    "        initrange = 0.5 / self.embed_size\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        '''\n",
    "        input_labels: 中心词, [batch_size]\n",
    "        pos_labels: 中心词周围 context window 出现过的单词 [batch_size * (window_size * 2)]\n",
    "        neg_labelss: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (window_size * 2 * K)]\n",
    "        return: loss, [batch_size]\n",
    "        '''\n",
    "        input_embedding = self.in_embed(input_labels) # B * embed_size\n",
    "        pos_embedding = self.out_embed(pos_labels) # B * (2*C) * embed_size\n",
    "        neg_embedding = self.out_embed(neg_labels) # B * (2*C * K) * embed_size\n",
    "        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(2)).squeeze() # B * (2*C)\n",
    "        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(2)).squeeze() # B * (2*C*K)\n",
    "        log_pos = F.logsigmoid(log_pos).sum(1)\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1) # batch_size\n",
    "        loss = log_pos + log_neg\n",
    "        return -loss\n",
    "    \n",
    "    def input_embeddings(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个模型以及把模型移动到GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataset(text, VOCAB_SIZE, word_to_idx, idx_to_word, word_freqs)\n",
    "dataloader = tud.DataLoader(dataset, \n",
    "                            batch_size = BATCH_SIZE, \n",
    "                            shuffle = True, \n",
    "                            num_workers = 0)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   61,     5,    22,     6,   786, 29999,   126,  7321,    37,  6701,\n",
       "         10888,    13,     7,    41,     5,    10,     0,     1,   548,    24,\n",
       "          2878,  4897,   812, 29999,     4, 29999,    12,  2258,     0, 10101,\n",
       "            34,   190,     1,   135,     1,    89,   150,  2384,  1053,   614,\n",
       "             6,   407,    10,    39,     1,   325,     5,     7,  5407, 11807,\n",
       "           750,    31,    16,  1389,   552,  2579,    68,  5214,     5,  1559,\n",
       "            85,     8,   431,  3730,   104,  2366,    15,     6,     0,  6480,\n",
       "            51,     1,  7494,    13,     3,     3,   181,  1873,   338,     2,\n",
       "            21, 21431,   301,    14,    28,   324,  1569,   136,   731,   625,\n",
       "             0,   703,    15,     8,  1734,    30,  1110,  4920,    25,   349,\n",
       "           742,  2322,     4,     7,   958, 29999,  7555,     3,     4,   345,\n",
       "             6,    67,     5,    52,     9,     0, 15525,   154,  4935,     0,\n",
       "          7185,     0,   115,     2,   566,     2,    64,     1]),\n",
       " tensor([[   21, 29999, 29999, 21448,   290,    72],\n",
       "         [ 3218,  2218,    11,  9035,    13, 29999],\n",
       "         [  947,   223, 29999,    20,     8,     9],\n",
       "         [ 1211,    28,  1571,  2201,    32,   431],\n",
       "         [28154, 20020,    14,     5,  2230,    13],\n",
       "         [    7,     7, 19586,   101,     1, 29999],\n",
       "         [   39,  6631,     6,     4,  2795,     0],\n",
       "         [ 3199,    37,     0,  6695, 20963,   325],\n",
       "         [    0,    57,   476,    40,     5,   523],\n",
       "         [  374,     5,   173,   403,     0,  1639],\n",
       "         [ 3172,   178, 10058, 20173, 15585,     9],\n",
       "         [ 7237,   183,  2642, 17863,     1,   183],\n",
       "         [    3,    12,    12,     3,     8,    22],\n",
       "         [   43,   201,  1990,   114,    46,   489],\n",
       "         [ 4503,    74,  1639,  1872,    28,    24],\n",
       "         [26411,  1747,    19,  1747,    19,    25],\n",
       "         [  283,   819,    25,   296,    86,     1],\n",
       "         [  100,    11, 11569,     5,  3134,     2],\n",
       "         [29999,     2,     0, 24601,  4719,    43],\n",
       "         [  479,  3964,   288,   531,     9,    15],\n",
       "         [   24,     0,  3073,    14,    66, 29276],\n",
       "         [29999,   220,   238,   220,   109,   109],\n",
       "         [   16, 29999, 29999,  5198,     4, 24797],\n",
       "         [29999,   487, 29999,     3,     8,    22],\n",
       "         [  841,     6,   726,   306,  1313,     2],\n",
       "         [    9,  1808,   409,     5,   254,   155],\n",
       "         [    1,     0,     3,    90,     2,     3],\n",
       "         [ 7539,     4,     0,   397, 29999,    84],\n",
       "         [    6, 11524,     1,  2162,    64,  1870],\n",
       "         [14996, 23952, 29999,   562,   239,   212],\n",
       "         [15377,    64, 15918,     0,   132,  1648],\n",
       "         [  866,    48,   122,    61,  9317,   803],\n",
       "         [29999, 11050, 29999,     0,  1964,    18],\n",
       "         [ 5944,  4286,   370,     2,   160,    13],\n",
       "         [   18,     5,   189,  4388,   100,   542],\n",
       "         [    2,    29,  3520,     1,    32,  3232],\n",
       "         [ 1938,     0,    16, 29999,   407,    10],\n",
       "         [ 5191, 29999,   166,     2,  7803,     3],\n",
       "         [    4, 17819,    13,  4095,    19,  1768],\n",
       "         [ 1006,     4,   825,  2624,    40,    78],\n",
       "         [   27, 21972,   673,     0, 29999,   313],\n",
       "         [ 5409,     6, 10318,    11, 29999,     0],\n",
       "         [   16,   435,    35,     0,    52,     4],\n",
       "         [ 4817, 18811,     2,    91, 29999,   142],\n",
       "         [   48,    28,    50,     0,  4151,     4],\n",
       "         [  325,    14,   632,   130,     0,    84],\n",
       "         [29999, 29999,    60,  6800,    27,     0],\n",
       "         [  292,   537,     9,     7,    15,    58],\n",
       "         [ 5191,  3960,  3274,    41,  2977,    59],\n",
       "         [  701,    13,  7760,    18,  1261,     2],\n",
       "         [    1,     0, 29999,    40,   941,    28],\n",
       "         [ 6623,    26,   161, 10691,    23,   159],\n",
       "         [   21,    12,     8,     7,     9,     7],\n",
       "         [  151,    61,  2218, 23864,  3386,     3],\n",
       "         [ 3228,    24,  3004,    46,   818,    47],\n",
       "         [    1,     0, 14676,   407,    10,   296],\n",
       "         [    5,   125,    26,    37,   841,     6],\n",
       "         [  583,  3621,     2,    23,  3587,     2],\n",
       "         [    4,   854, 16303,   801,   523,     1],\n",
       "         [   30, 26735,     1,  7725,  6263,    37],\n",
       "         [    9,  1990,  1636,     0,  1667,     1],\n",
       "         [   16,     7,   129,    12,  2076,   626],\n",
       "         [ 2114,  4818,  1260, 29999,  1914,   776],\n",
       "         [   22,    16,     7,  1213,  6233,    22],\n",
       "         [ 3843,    26,     4,   215, 17577,    32],\n",
       "         [12128, 16052,     2,     0,  2373,     1],\n",
       "         [    4,   305,     3,     7,     7,     3],\n",
       "         [    7,    14,   233,     5,  5310,     4],\n",
       "         [21379,  4445, 29999,   179,     2,     0],\n",
       "         [ 4462,    33,   536,     1,  1298,     2],\n",
       "         [   89,    32,   734,    31,  5135,     4],\n",
       "         [   49,  8468,  4696,  8935,    54,    11],\n",
       "         [    0,  1177,     6,     0,   975,    77],\n",
       "         [  381,   803,  4397,    21,  2011,   118],\n",
       "         [  550,  1542,    72,    12,    12,     3],\n",
       "         [   75,    16,   469,     8,     8,     9],\n",
       "         [   12,    15, 12236,    97,     0,   255],\n",
       "         [11882,     1, 29999,     5,   775,   686],\n",
       "         [   58,    10,     5,     1,     0,  1581],\n",
       "         [    4,   549,   423,  1865,  1774,     4],\n",
       "         [   27,     3,     8,     7,     3,     8],\n",
       "         [29999,  6939,   292, 16628, 29999,   593],\n",
       "         [   34,  2140,   946,     2,   141,   117],\n",
       "         [29999, 29999,  2158,  1430,    72,     3],\n",
       "         [ 2616,  2316,  6345,  6992, 14423,    77],\n",
       "         [    7,     7,    12,    18, 10100, 29999],\n",
       "         [   98,  9583,  2845,   787,  4976,   123],\n",
       "         [    2,    29,   579,  5588,  4417,     5],\n",
       "         [    0,  1363, 29999,     2, 29999, 29999],\n",
       "         [ 1860, 12421,     2,  2638,     1, 29999],\n",
       "         [    0,  1588,     1,   245,   271,  2190],\n",
       "         [   96, 12706,     0,     1,     0,  1943],\n",
       "         [    3,    12,     3,   992,     1,  3761],\n",
       "         [    4,    29,     3,    22,    15,   192],\n",
       "         [   36,   884,   746,     0,   572,    11],\n",
       "         [   37,  3334,    11,  6033,  9078,     6],\n",
       "         [    6,  3598,     0,    93,    70,   161],\n",
       "         [  240,    33,  3985,     4,    56,    42],\n",
       "         [ 1328,     2,  7670,    37,  4555,     6],\n",
       "         [   99,   284,  4734,     5,  1448,  2188],\n",
       "         [  198,    18,     0,  1707,  1488,    24],\n",
       "         [14749,  1267,     5,     4,    43,   201],\n",
       "         [  452,    39,  1377,     3,     8,    16],\n",
       "         [   16,     3,     3,     7,     8,    22],\n",
       "         [  222,  6250,    25, 13247,    23,  1066],\n",
       "         [  413,    25,     0, 29999,    28, 26225],\n",
       "         [   36,  2289,     0,     6,    29,  3572],\n",
       "         [    0,    84,    80,    12,    16,    21],\n",
       "         [   29, 17769, 29999,     0,    45, 25074],\n",
       "         [   25,   482,     7,  3015,    41,  6857],\n",
       "         [   13,     5,  5639,     0,   148,   240],\n",
       "         [    3,    15,  2673,  1376,     1,  1072],\n",
       "         [    6,     6, 12364,  4102,   572,  3570],\n",
       "         [ 1334,   175,  3941,   332,     4,     0],\n",
       "         [    0,     3,     8,     7,    14,     6],\n",
       "         [29999,  4006,    11, 29999,    17, 12891],\n",
       "         [  273,   115,     6,    29,   946,   741],\n",
       "         [    5,  2465,  1514,   139,     0,  1915],\n",
       "         [29999,     2, 29999,    12, 29999, 29999],\n",
       "         [29999,   317,    18,   816, 29999, 26675],\n",
       "         [    2,   305,    23,     2,  1582,  4125],\n",
       "         [ 8165,  1758,    27,     3,     8,     9],\n",
       "         [   17,   420,    27,    41,    17, 15380],\n",
       "         [  287,     4, 23974,   103,    59,     6],\n",
       "         [  237,   505,   342,  2672,     4,     0],\n",
       "         [    0,  1613,  1134,     0,  2736,   351],\n",
       "         [ 8763,    44,  4591,    44,  1652,    14],\n",
       "         [  612,  3616,   172,  1311,  1721,    32]]),\n",
       " tensor([[ 3224,    19,    21,  ..., 29563,   905,  2587],\n",
       "         [  658,   119,    14,  ...,  1647,    75,  3094],\n",
       "         [18400,  4072,    54,  ..., 25309,   241, 14110],\n",
       "         ...,\n",
       "         [  362,  5323,  6610,  ..., 10439,  1423,   112],\n",
       "         [29999,  6507,  1808,  ..., 18329,   908,   463],\n",
       "         [13963,     9,  3727,  ...,  1336, 19302,    49]])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是评估模型的代码，以及训练模型的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     19
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate(filename, embedding_weights): \n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename, sep=\",\")\n",
    "    else:\n",
    "        data = pd.read_csv(filename, sep=\"\\t\")\n",
    "    human_similarity = []\n",
    "    model_similarity = []\n",
    "    for i in data.iloc[:, 0:2].index:\n",
    "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
    "        if word1 not in word_to_idx or word2 not in word_to_idx:\n",
    "            continue\n",
    "        else:\n",
    "            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
    "            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]\n",
    "            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))\n",
    "            human_similarity.append(float(data.iloc[i, 2]))\n",
    "\n",
    "    return scipy.stats.spearmanr(human_similarity, model_similarity)# , model_similarity\n",
    "\n",
    "def find_nearest(word):\n",
    "    index = word_to_idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "    return [idx_to_word[i] for i in cos_dis.argsort()[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型：\n",
    "- 模型一般需要训练若干个epoch\n",
    "- 每个epoch我们都把所有的数据分成若干个batch\n",
    "- 把每个batch的输入和输出都包装成cuda tensor\n",
    "- forward pass，通过输入的句子预测每个单词的下一个单词\n",
    "- 用模型的预测和正确的下一个单词计算cross entropy loss\n",
    "- 清空模型当前gradient\n",
    "- backward pass\n",
    "- 更新模型参数\n",
    "- 每隔一定的iteration输出模型在当前iteration的loss，以及在验证数据集上做模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     16,
     21
    ],
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-48f364312fcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mUSE_CUDA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0minput_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE)\n",
    "for e in range(NUM_EPOCHS):\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        if USE_CUDA:\n",
    "            input_labels = input_labels.cuda()\n",
    "            pos_labels = pos_labels.cuda()\n",
    "            neg_labels = neg_labels.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            with open(LOG_FILE, \"a\") as fout:\n",
    "                fout.write(\"epoch: {}, iter: {}, loss: {}\\n\".format(e, i, loss.item()))\n",
    "                print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))\n",
    "            \n",
    "        if i % 5000 == 0:\n",
    "            embedding_weights = model.input_embeddings()\n",
    "            sim_simlex = evaluate(SIMLEX_PATH, embedding_weights)\n",
    "            sim_men = evaluate(MEN_PATH, embedding_weights)\n",
    "            sim_353 = evaluate(SIM353_PATH, embedding_weights)\n",
    "            with open(LOG_FILE, \"a\") as fout:\n",
    "                print(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "                    e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "                fout.write(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "                    e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "                \n",
    "#     embedding_weights = model.input_embeddings()\n",
    "    torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"embedding-{}.th\".format(EMBEDDING_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 MEN 和 Simplex-999 数据集上做评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simlex-999 SpearmanrResult(correlation=0.23862807852014967, pvalue=8.082212007686697e-14)\n",
      "men SpearmanrResult(correlation=0.3879450295711117, pvalue=1.6836442672726613e-93)\n",
      "wordsim353 SpearmanrResult(correlation=0.4607564965641069, pvalue=3.598974422141974e-18)\n"
     ]
    }
   ],
   "source": [
    "embedding_weights = model.input_embeddings()\n",
    "print(\"simlex-999\", evaluate(\"simlex-999.txt\", embedding_weights))\n",
    "print(\"men\", evaluate(\"men.txt\", embedding_weights))\n",
    "print(\"wordsim353\", evaluate(\"wordsim353.csv\", embedding_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 寻找nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good ['good', 'bad', 'luck', 'quick', 'safe', 'loving', 'happy', 'nice', 'fun', 'pretty']\n",
      "fresh ['fresh', 'grazing', 'liquor', 'brackish', 'cloth', 'grain', 'rotting', 'smoke', 'clean', 'frozen']\n",
      "monster ['monster', 'ness', 'giant', 'loch', 'creature', 'beast', 'hammer', 'serpent', 'snake', 'killer']\n",
      "green ['green', 'yellow', 'blue', 'gray', 'purple', 'colored', 'pink', 'orange', 'white', 'grey']\n",
      "like ['like', 'resemble', 'resembling', 'including', 'similarly', 'exotic', 'such', 'unlike', 'eg', 'etc']\n",
      "america ['america', 'europe', 'carolina', 'americas', 'asia', 'africa', 'dakota', 'oceania', 'cherokee', 'australia']\n",
      "chicago ['chicago', 'illinois', 'boston', 'detroit', 'atlanta', 'cincinnati', 'milwaukee', 'cleveland', 'baltimore', 'philadelphia']\n",
      "work ['work', 'composing', 'filmmaking', 'works', 'compose', 'inventions', 'inventing', 'experimentation', 'pioneering', 'journalistic']\n",
      "computer ['computer', 'computers', 'computing', 'hardware', 'graphics', 'programmable', 'electronics', 'machines', 'programmer', 'console']\n",
      "language ['language', 'languages', 'dialect', 'dialects', 'vocabulary', 'phonology', 'syntax', 'orthography', 'lexicon', 'ido']\n"
     ]
    }
   ],
   "source": [
    "for word in [\"good\", \"fresh\", \"monster\", \"green\", \"like\", \"america\", \"chicago\", \"work\", \"computer\", \"language\"]:\n",
    "    print(word, find_nearest(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单词之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n",
      "throne\n",
      "princess\n",
      "son\n",
      "emperor\n",
      "constantine\n",
      "vii\n",
      "claudius\n",
      "daughter\n",
      "charlemagne\n",
      "heir\n",
      "prince\n",
      "eldest\n",
      "ruler\n",
      "isabella\n",
      "tudor\n",
      "empress\n",
      "kings\n",
      "augustus\n",
      "baldwin\n"
     ]
    }
   ],
   "source": [
    "man_idx = word_to_idx[\"man\"] \n",
    "king_idx = word_to_idx[\"king\"] \n",
    "woman_idx = word_to_idx[\"woman\"]\n",
    "embedding = embedding_weights[woman_idx] - embedding_weights[man_idx] + embedding_weights[king_idx]\n",
    "cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "for i in cos_dis.argsort()[:20]:\n",
    "    print(idx_to_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
